# -*- coding: utf-8 -*-
"""Copy of MTD_ABAC Staff - Correlated_Attributes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUln7DI9ot2UN8B_Wyu7ZTYstCBvZrvf
"""

pip install pyspark

pip install fpgrowth_py

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

cd drive/MyDrive/Colab Notebooks

# Essential libraries
import pandas as pd
import numpy as np
import random
import time
from statistics import mean, variance, median, stdev
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.cluster.hierarchy as shc

# Scikit-learn libraries
from sklearn import datasets
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import davies_bouldin_score, silhouette_score
from sklearn.preprocessing import LabelEncoder, StandardScaler, normalize
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# Keras libraries (if deep learning is used)
from keras.models import Model
from keras.layers import Dense, Input
from keras.preprocessing import sequence

# Loading dataset (example: Iris dataset)
from sklearn.datasets import load_iris

# Initialize scaler
scaler = StandardScaler()

dataset_to_use = pd.read_csv('mtd_datasets/StaffAssign18.csv', index_col=False, dtype='unicode')
dataset_to_use

dataset_to_use = dataset_to_use[['CountyName', 'DistrictName',
       'SchoolName', 'ClassID', 'CourseCode']]
dataset_to_use



dataset_to_use = dataset_to_use.iloc[1:40000, :]





import warnings
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import fpgrowth, association_rules

# Suppress specific DeprecationWarnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

columns = ['CountyName', 'DistrictName', 'ClassID']

# Step 1: Focus on 'districtwide' and 'countyname' columns
df_filtered = dataset_to_use[columns]

# Step 1: Convert each row to a transaction format
transactions = df_filtered.apply(lambda x: list(x.dropna().astype(str)), axis=1).tolist()

# Step 2: One-hot encode the transactions
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
transaction_df = pd.DataFrame(te_ary, columns=te.columns_)

# Step 3: Apply FP-Growth to find frequent itemsets
frequent_itemsets = fpgrowth(transaction_df, min_support=0.0001, use_colnames=True)

# Step 4: Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.0001)

# Step 5: Create a dictionary to track correlated attribute sets for each column
correlated_attributes = {col: set() for col in columns}

# Step 6: Populate correlated attributes for each column
for _, row in rules.iterrows():
    antecedents = list(row['antecedents'])
    consequents = list(row['consequents'])

    # Check if antecedents and consequents are from the same column
    for col in columns:
        col_antecedents = [item for item in antecedents if item in dataset_to_use[col].astype(str).unique()]
        col_consequents = [item for item in consequents if item in dataset_to_use[col].astype(str).unique()]

        # If the antecedents and consequents are in the same column, consider them correlated
        if col_antecedents and col_consequents:
            correlated_attributes[col].update(col_antecedents + col_consequents)

# Step 7: Output correlated attribute sets for each column
for col, attr_set in correlated_attributes.items():
    print(f"Correlated attributes for {col}: {attr_set}")